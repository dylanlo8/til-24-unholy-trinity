{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a76decb-b069-4890-bf43-66f9c7637f52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import ImageReadMode, read_image\n",
    "from torchvision.transforms import CenterCrop, ConvertImageDtype, Normalize, Resize\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "# from torchvision.transforms.functional as F\n",
    "import torch.optim as optim  # This import is necessary for using the optim.Adam optimizer\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    CLIPModel,\n",
    "    CLIPTokenizer,\n",
    "    CLIPProcessor,\n",
    "    CLIPImageProcessor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc0d187-06e9-4c31-9e1a-6aab02e2d27f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# TO ADD :\n",
    "# Gradient Checkpointing\n",
    "# Filter out bias from weight decay\n",
    "# Decaying learning rate with cosine schedule\n",
    "# Half-precision Adam statistics\n",
    "# Half-precision stochastically rounded text encoder weights were used\n",
    "\n",
    "# #BATCH_SIZE must larger than 1\n",
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00098d62-f1e1-4a22-b86b-5d31a6d0fbaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [] \n",
    "with open('../../../novice/vlm.jsonl', 'r') as file: \n",
    "    for line in file: \n",
    "        data.append(json.loads(line)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dddd66cd-9830-40b0-94a0-ec92a68aa22b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_image_path = []\n",
    "list_txt = []\n",
    "list_bboxes = []\n",
    "\n",
    "base_path = '../../../novice/images/'\n",
    "# Extract information from data\n",
    "for entry in data:\n",
    "    image_path = base_path + entry['image']\n",
    "    for annotation in entry['annotations']:\n",
    "        list_image_path.append(image_path)\n",
    "        list_txt.append(annotation['caption'])\n",
    "        list_bboxes.append(annotation['bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aede645-eb9f-49b2-a79c-3ffddcc456cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4de62b08-119b-4e42-a076-607628f46477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class image_title_dataset(Dataset):\n",
    "    def __init__(self, list_image_path, list_txt, list_bboxes):\n",
    "\n",
    "        self.image_path = list_image_path\n",
    "        self.title  = clip.tokenize(list_txt) #you can tokenize everything at once in here(slow at the beginning), or tokenize it in the training loop.\n",
    "        self.bboxes = list_bboxes  # List of bounding boxes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # open image and crop it\n",
    "        with Image.open(self.image_path[idx]) as img:\n",
    "            bbox = self.bboxes[idx]\n",
    "            left = bbox[0]\n",
    "            top = bbox[1]\n",
    "            width = bbox[2] + 5\n",
    "            height = bbox[3] + 5\n",
    "\n",
    "            right = left + width\n",
    "            bottom = top + height\n",
    "\n",
    "            img_cropped = img.crop((left, top, right, bottom))  # Crop image based on the bounding box\n",
    "\n",
    "        image = preprocess(img_cropped) # Image from PIL module\n",
    "        title = self.title[idx]\n",
    "        return image,title\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 5\n",
    "\n",
    "dataset = image_title_dataset(list_image_path,list_txt, list_bboxes)\n",
    "train_dataloader = DataLoader(dataset,batch_size = BATCH_SIZE) #Define your own dataloader\n",
    "\n",
    "if device == \"cpu\":\n",
    "    model.float()\n",
    "else:\n",
    "    clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
    "    \n",
    "# # Test the DataLoader output\n",
    "# sample_batch = next(iter(train_dataloader))\n",
    "# print(sample_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32842696-fbbd-4580-bd3d-9e209fc0ef12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c74deb53-c0a1-4e33-8878-b477843cc40f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f13bb81-6e49-484d-8c45-823430985e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2b4565c3117e7934\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2b4565c3117e7934\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard --logdir /runs --load_fast true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f23edb65-7b95-4656-851e-d8a177f1b09b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/5, Loss: 0.3540: 100%|██████████| 234/234 [11:24<00:00,  2.92s/it]\n",
      "Epoch 1/5, Loss: 0.2395: 100%|██████████| 234/234 [05:31<00:00,  1.42s/it]\n",
      "Epoch 2/5, Loss: 0.2476: 100%|██████████| 234/234 [05:41<00:00,  1.46s/it]\n",
      "Epoch 3/5, Loss: 0.2339: 100%|██████████| 234/234 [05:43<00:00,  1.47s/it]\n",
      "Epoch 4/5, Loss: 0.2180: 100%|██████████| 234/234 [05:37<00:00,  1.44s/it]\n"
     ]
    }
   ],
   "source": [
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-6,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "# add your own code to track the training progress.\n",
    "for epoch in range(EPOCH):\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    for batch in pbar:\n",
    "        # zero the parameter gradients \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images,texts = batch \n",
    "\n",
    "        images= images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        # Compute loss\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        running_loss += total_loss.item()\n",
    "        \n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else: \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "        \n",
    "        pbar.set_description(f\"Epoch {epoch}/{EPOCH}, Loss: {total_loss.item():.4f}\")\n",
    "        writer.add_scalar(\"Loss x epoch\", running_loss/len(train_dataloader), epoch)\n",
    "        \n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac6105d6-6737-4110-bf9d-02ac02e596b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, f\"clip-finetune/clip.pt\") #just change to your preferred folder/filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e9c85-ed3f-4716-9fd4-f791c2b6d9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
    "checkpoint = torch.load(\"clip-finetune/clip.pt\")\n",
    "\n",
    "# # Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "# # checkpoint['model_state_dict'][\"input_resolution\"] = model.input_resolution #default is 224\n",
    "# checkpoint['model_state_dict'][\"context_length\"] = model.context_length # default is 77\n",
    "# checkpoint['model_state_dict'][\"vocab_size\"] = model.vocab_size \n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c45525-8c72-4a7b-b6e6-74d15785de49",
   "metadata": {},
   "source": [
    "## Alternative Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a665f9-d725-46a6-9838-3cd432c25b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Callable\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from torch.nn import functional as F\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from PIL import Image\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "from transformers import FlaxCLIPModel, CLIPProcessor, TrainingArguments, is_tensorboard_available\n",
    "from tqdm import tqdm\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=\"openai/clip-vit-base-patch32\",\n",
    "        metadata={\"help\": \"The model checkpoint for weights initialization.\"}\n",
    "    )\n",
    "    dtype: Optional[str] = field(\n",
    "        default=\"float32\", \n",
    "        metadata={\"help\": \"Floating-point format in which the model weights should be initialized.\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    data_dir: str = field(\n",
    "        default=\"./data\",\n",
    "        metadata={\"help\": \"Path to the directory containing the data files.\"}\n",
    "    )\n",
    "\n",
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def forward(self, img, bbox):\n",
    "        \"\"\" Resize image keeping aspect ratio and apply transforms \"\"\"\n",
    "        img = F.crop(img, *bbox)  # Crop image to bounding box\n",
    "        img = F.resize(img, [self.image_size, self.image_size], interpolation=Image.BICUBIC)\n",
    "        img = F.to_tensor(img)\n",
    "        img = F.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        return img\n",
    "\n",
    "class ImageTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, json_file_path, transform=None):\n",
    "        self.json_file_path = json_file_path\n",
    "        self.transform = transform\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\" Load data from JSONL file and transform into single caption per item \"\"\"\n",
    "        data = []\n",
    "        with open(self.json_file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                entry = json.loads(line)\n",
    "                for annotation in entry['annotations']:\n",
    "                    data.append({\n",
    "                        'image_path': os.path.join(self.json_file_path, entry['image']),\n",
    "                        'caption': annotation['caption'],\n",
    "                        'bbox': annotation['bbox']  # [top, left, width, height]\n",
    "                    })\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image_path = item['image_path']\n",
    "        image = read_image(image_path, mode=ImageReadMode.RGB)\n",
    "        bbox = item['bbox']  # Assume bbox format is [top, left, width, height]\n",
    "        if self.transform:\n",
    "            image = self.transform(image, bbox)\n",
    "        caption = item['caption']\n",
    "        return image, caption\n",
    "\n",
    "def main():\n",
    "    model_args = ModelArguments()\n",
    "    data_args = DataTrainingArguments(data_dir=\"../../../novice/images/vlm.jsonl\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./training_output\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=32,\n",
    "        save_steps=10,\n",
    "        save_total_limit=5\n",
    "    )\n",
    "\n",
    "    transform = Transform(image_size=224)\n",
    "    dataset = ImageTextDataset(data_args.data_dir, transform=transform)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=training_args.per_device_train_batch_size, shuffle=True)\n",
    "\n",
    "    model = FlaxCLIPModel.from_pretrained(model_args.model_name_or_path, dtype=getattr(jnp, model_args.dtype))\n",
    "    state = train_state.TrainState.create(apply_fn=model.__call__, params=model.params, tx=optax.adam(5e-5))\n",
    "\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    for epoch in range(training_args.num_train_epochs):\n",
    "        for batch in tqdm(data_loader):\n",
    "            state, loss = train_step(state, batch, rng)\n",
    "            logger.info(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
